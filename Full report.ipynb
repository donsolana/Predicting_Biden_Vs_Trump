{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Biden vs. Trump: An Empirical Bayesian Approach.**\n\n## Introduction \nIn 2016 Donald John Trump became the president of the United States against all odds, many prominent election research organisations gave the then contender Hillary Clinton a higher chance of winning, with confident predictions in favor of the former secretary of state ranging around eighty to ninety percent, nevertheless Donald Trump emerged as the winner. \n\n### Bayesian Statistics\nThe 2016 presidential election is proof that elections can be notoriously difficult to predict acurately. However, certain techniques such as Bayesian statistics has been known to produce more accurate results. The core idea behind bayesian methods is to update an uninformed/best-guess judgement based on new information/data. \n\n### Assumptions\n1. A key assumption is that the distribution of votes in the population and its sample is aproximately normal. This of course comes with its own set of assumptions. for more click on [this link](https://en.wikipedia.org/wiki/Normal_distribution)\n\n\n### Polling Data\nThe data we use to model the election was gotten from opinion polls conducted by various organisations. It was sourced from FiveThirtyEight a popular data journalism platform. A little cleaning will be neccesary to ensure the integrity of the data.\n\n"},{"metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#load required packages\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(dslabs)\n\n\n#read in the data \nus_polls_2020 <- read_csv(\"../input/presidential-polls-source-fivethirtyeight/president_polls (1).csv\")\n \nnames(us_polls_2020)\n#parse end_date as a date format\nus_polls_2020$end_date <- mdy(us_polls_2020$end_date)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filter the data for polls with a rating \"B-\" rating and above,\n#and polls that end on the \"2/11/2020\" or later.\n#then select relevant columns using their indici\n#then select completed polls by filtering for observations with the max sample size\n\nus_polls <- us_polls_2020 %>%  \n  select(2,4,6,8,12,13,16,20,21,27,34,36,38, url) %>%\n  group_by(poll_id) %>% \n  filter(end_date == max(end_date) & sample_size == max(sample_size) & !duplicated(answer, fromLast = FALSE)) %>% \n  ungroup()\n  \n#remove any duplications and spread the answer column based on \"pct\"\nus_polls_tidish <- us_polls[!duplicated(us_polls, fromLast = FALSE),] %>%\n               spread(answer, pct) \nhead(us_polls_tidish)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data set above has been filtered to remove duplicated values and incomplete polling results, however to bring the data to complete \"tidy\" status, we must ensure that each contains a unique observation(poll) while each feature as a unique column; here the observed percentage for each candidate can be considered a \"feature\". Reshaping the data into this format will allow for easier computation later on.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create one row per poll\nus_polls_tidy <- us_polls_tidish %>% \n  group_by(poll_id) %>%\n  summarize(state = unique(state),\n            pollster = unique(pollster),\n            pollster_grade =  unique(fte_grade),\n            sample_size = unique(sample_size), \n            end_date = unique(end_date),\n            Biden_prop = sum(ifelse(!is.na(Biden), Biden/100, 0)),\n            Trump_prop = sum(ifelse(!is.na(Trump), Trump/100, 0)),\n            West_prop = sum(ifelse(!is.na(West),West/100,0)),\n            Hawkins_prop = sum(ifelse(!is.na(Hawkins), Hawkins/100,0)),\n            Jorgensen_prop = sum(ifelse(!is.na(Jorgensen), Jorgensen/100, 0))\n            )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now that the data is tidy we can commence modelling."},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot biden and trump proportions over time\nggplot(us_polls_tidy, aes(end_date, Biden_prop * 100)) +\n  geom_point(colour = \"blue\") +\n  geom_point(aes(end_date, Trump_prop * 100), colour = \"red\") +\n   theme(axis.line = element_line(colour = \"black\"),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.border = element_blank(),\n    panel.background = element_blank()) +\n  xlab(\"Time\") +\n  ylab(\"Poll Results\") +\n  labs(title = \"Biden Vs. Trump\",\n       subtitle = \"Respondent Support over Time\")\n\n# We can see here that Biden(blue) seems to be polling higher than trump \n#The data also shows that four data points report missing values for Biden","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modelling The Outcome\nEarlier, I mentioned bayesian statistics, here i will go into futher detail. As i said earlier bayesian methods enable us to make decisions by factoring new information, as such, this methods allow us to model several levels of varibility as we would \"new information\". Here and throughout the analysis we would assume that the outcome of the elections will not be affected by other candidates other than Trump and Biden.\n\n\nFor the purpose of this analysis we would denote the true proportion of Biden voters as $p$ and the spread as: $d$. We can make a priori(uninformed) estimate of the spread, denoted as $\\mu$. We assume that $\\mu$ which is an priori estimate of the spread is assumed to be approximately normal with a standard error of $\\tau^2$. Represented as, \n\n$$ d\\ \\sim\\ N(\\mu,\\ \\tau^2).$$ \n\nWhile the following formular represents randomness due to sampling and pollster effect:\n\n$$ Y|d \\sim N(d,\\ \\sigma^2).$$\n\n\nThe spread is the difference between the proportions observed in our data, this can be denoted mathematically as:         \n$$ d = p\\ -\\ (1-p),$$\n$$ d = 2p\\ - \\ 1.$$\n\nTherefore, p can be derived as,\n$$ p = \\frac {d + 1} {2} $$\n\nwe can derive an estimate of $p$ from the polling data represented as $\\hat X$, and use this to compute a standard deviation of the observed data.\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Simulating the Outcome\nThe mathetical model,  \n$ X_{i,j} = d + b + h_i + \\epsilon_{i,j} $ represents a \n\nWhere,\n* The index  $i$  represents the different pollsters.\n* The index  $j$  represents the different polls\n* $X_{i,j}$  is the  $j$ th poll by the  i th pollster \n* $d$  is the actual spread of the election\n* $b$  is the general bias affecting all pollsters\n* $h_i$  represents the house effect for the  i th pollster\n* $Ïµ_{i,j}$ represents the random error associated with the  i,j th poll"},{"metadata":{"trusted":true},"cell_type":"code","source":"#compute se, spread and standard error\nmu <- 0 #best quess spread\ntau <- 0.02 # best quess standard deviation\nset.seed(2020-11-04)\n\nus_polls_tidier <-  us_polls_tidy[,1:8] %>%  \n  filter(state != is.na(state) &\n         end_date >= \"2020-10-31\"\n        ) %>%\n  mutate(j = length(unique(pollster)),\n         spread = Biden_prop - Trump_prop,\n         p_hat = (spread + 1)/2,\n         se_xhat = 2 * sqrt(p_hat * (1-p_hat)/sample_size))%>%\n  ungroup() %>% \n  group_by(state) %>%\n  summarize(N = n(),\n            avg_spread = mean(spread),\n            sd = sd(spread)) %>%\n   mutate( median_se = median(sd, na.rm = TRUE), \n           se = ifelse(is.na(sd),median_se , sd),\n            sigma =  sqrt(se/sqrt(N)^2 + .025^2),\n            B =  sigma^2 / (sigma^2 + tau^2),\n            posterior_mean = B*mu + (1-B)*avg_spread,\n            posterior_se =  sqrt(1 / (1/sigma^2 + 1/tau^2)),\n            upper_ci = posterior_mean + qnorm(0.975)* posterior_se,\n            lower_ci = posterior_mean - qnorm(0.975)* posterior_se,\n            pct = 1 - pnorm(0, posterior_mean, posterior_se))\n\nelectoral_vote <- results_us_election_2016[,1:2]\nus_polls_final <- left_join(us_polls_tidier, electoral_vote, by = \"state\")\nBiden_votes <- replicate(10000, {\n    us_polls_final %>% mutate(\n                       simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se),\n                       Biden = ifelse(simulated_result > 0, electoral_votes, 0)) %>%    # award votes if Biden wins state\n                      summarize(Biden = sum(Biden, na.rm = TRUE)) %>%    # total votes for Biden\n                         .$Biden   \n})\nmean(Biden_votes> 269)    \n           \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\nHere we see that Biden wins about 80% of the time in our simulation of 10000 iterations.\n\n#### ***Note***\nPredicting elections is not straight forward, in 2016 most people predicted that Hillary clinton will with overwhelming odds, technically speaking you can only call an election if it falls within a predetermined confidence interval. Famous mathematians and quantitative analyst have long quarrelled about the soundness of the method. Nevertheless, this method or a slightly more complex variation of it is championed by top poll agregators."}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}
